{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "T20_L--BA--Back_Propagation_Lecture_Notes.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/schwaaweb/Data-Structures/blob/master/TT20_L--BA--Back_Propagation_Lecture_Notes.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "1gqyHZ0y-dhs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Part I: Calculus \n",
        "\n",
        "> 1) The Partial Derivative\n",
        "\n",
        "> 2) The Gradient\n",
        "\n",
        "> 3) Chain Rule\n",
        "\n",
        "\n",
        "###Part II: Computing the Gradient of Neural Networks\n",
        "\n",
        "> 1) Derivatives of Common Activation Functions\n",
        "\n",
        ">*   Sigmoid\n",
        "*   rectified linear unit (ReLU)\n",
        "*   Tanh\n",
        "\n",
        "> 2) Cost Function as a Composition of Activation Functions\n",
        "\n",
        "> 3) The Gradient of a Neural network\n",
        "\n",
        "###Part III: Gradient Descent\n",
        "\n",
        "> 1) Use the Gradient to Find a Minimum\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "zTP7JERek981",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "Back Propagation has a fairly simple problem state. Take a cost function, something like:\n",
        "\n",
        "$\\epsilon = \\sum_i^n (y_i - \\hat{y}_i(\\beta))^2$\n",
        "\n",
        "And find the weights $\\beta$ that yield its minimum. I've written $\\hat{y}(\\beta)$ to denote that our predictions $\\hat{y}$ are functions of our weights $\\beta$. This is both a nitpicky and crucial point, as we will confirm later. \n",
        "\n",
        "The goal of this lecture is to give a concrete, nuts-and-bolts conception of exactly what is involved in accomplishing this.\n"
      ]
    },
    {
      "metadata": {
        "id": "XOfUA8WD8G_h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part I: Calculus"
      ]
    },
    {
      "metadata": {
        "id": "fM5mm7VvglHu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The Partial Derivative\n",
        "\n",
        "According to Google the partial derivative: *is a derivative of a function of two or more variables with respect to one variable, the other(s) being treated as constant.*\n",
        "\n",
        "Example 1: \n",
        "\n",
        "> $f(x,y)=x^2+y^2$\n",
        "\n",
        "> $\\frac{\\partial f(x,y)}{\\partial x} = 2x + 0 = 2x$\n",
        "\n",
        "> $\\frac{\\partial f(x,y)}{\\partial y} = 0 + 2y = 2y$\n",
        "\n",
        "Practice Problem 1:\n",
        "\n",
        "> $g(x,y) = \\sin(x)+y$\n",
        "\n",
        "> $\\frac{\\partial g(x,y)}{\\partial x} = \\cos(x)$\n",
        "\n",
        "> $\\frac{\\partial g(x,y)}{\\partial y} =1$\n",
        "\n",
        "Practice Problem 2:*\n",
        "\n",
        "> $h(x,y) = sin^2(x)y^3$\n",
        "\n",
        "> $\\frac{\\partial h(x,y)}{\\partial x} = $\n",
        "\n",
        "> $\\frac{\\partial h(x,y)}{\\partial y} = $\n",
        "\n",
        "**hint: we need chain rule!*\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "PQ6Rk2DMm128",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The Gradient\n",
        "\n",
        "According to Kalid Azad (author of the web page Better Explained), he succinctly defines the gradient as the thing that \"points in the direction of greatest increase of a function.\" The word \"direction\" here is key: the gradient is a **vector** whose entries are found (nay, computed!) by taking the partial derivatives with respect to the relevant independent variables.\n",
        "\n",
        "$\\nabla f(x,y) = (\\frac{\\partial f(x,y)}{\\partial x}, \\frac{\\partial f(x,y)}{\\partial y}) = (2x, 2y) = \\begin{bmatrix} 2x \\\\ 2y \\end{bmatrix} $"
      ]
    },
    {
      "metadata": {
        "id": "J_ZW_54Mq1H8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Chain Rule\n",
        "\n",
        "Chain rule allows us to find the derivative of a composition of functions. A composition of functions, might playout something like this: \n",
        "\n",
        "Suppose\n",
        "\n",
        "$f(x) = x^2$\n",
        "\n",
        "$g(x) = \\sin(x)$\n",
        "\n",
        "$(f \\circ g)(x) = f(g(x)) = \\sin^2(x)$\n",
        "\n",
        "How do we take the derivative of this function? \n",
        "\n",
        "**Chain Rule States:** \n",
        "\n",
        "$(f \\circ g)'(x) = f'(g(x))g'(x)$\n",
        "\n",
        "This makes life so easy. We compute $f'(x)$ and $g'(x)$ separately:\n",
        "\n",
        "$f'(x) = 2x$\n",
        "\n",
        "$g'(x) = \\cos(x)$\n",
        "\n",
        "Then we just put everything together:\n",
        "\n",
        "$(f \\circ g)'(x) = 2\\big( \\sin (x) \\big) \\cos (x)$\n",
        "\n",
        "We can restate everything we just did again, but this time in Leibniz notation. This is the same notation we used when discussing partial derivatives above:  \n",
        "\n",
        "given: \n",
        "\n",
        "$y = u^2$\n",
        "\n",
        "$u = \\sin(x)$\n",
        "\n",
        "**Chain Rule Restated:** \n",
        "\n",
        "$\\frac{\\mathrm{d} y}{\\mathrm{d} x} =  \\frac{\\mathrm{d} y}{\\mathrm{d} u} \\frac{\\mathrm{d} u}{\\mathrm{d} x}$\n",
        "\n",
        "Notice how we don't have to go through the trouble of writing a nested function term like $f'(g(x))$.  \n",
        "\n",
        "$ \\frac{\\mathrm{d} v}{\\mathrm{d} u} = 2u$\n",
        "\n",
        "$ \\frac{\\mathrm{d} u}{\\mathrm{d} x} = \\cos(x)$\n",
        "\n",
        "$\\frac{\\mathrm{d} y}{\\mathrm{d} x} = 2 u \\cos(x) =  2\\big( \\sin (x) \\big) \\cos (x)$\n",
        "\n",
        "With this more compact notation, we can conceivably express the derivative for the composition of arbitrarily many functions. Say we were also given:\n",
        "\n",
        "$z = e^{2w}$\n",
        "\n",
        "$w = \\sqrt{y}$\n",
        "\n",
        "then $\\frac{\\mathrm{d} z}{\\mathrm{d} x}$ is just:\n",
        "\n",
        "$\\frac{\\mathrm{d} z}{\\mathrm{d} x} = \\frac{\\mathrm{d} z}{\\mathrm{d} w}\\frac{\\mathrm{d} w}{\\mathrm{d} y}\\frac{\\mathrm{d} y}{\\mathrm{d} u}\\frac{\\mathrm{d} u}{\\mathrm{d} x}$\n",
        "\n",
        "*if* we weren't too interested in substituting back in expressing everything in terms of $x$ (hint: when back propagating, we won't be), this evaluates to:\n",
        "\n",
        "\n",
        "$\\frac{\\mathrm{d} z}{\\mathrm{d} x} = \\big( 2e^{2w}\\big)\\big( \\frac{1}{2\\sqrt{y}}\\big)\\big( 2u\\big)\\big( cos(x)\\big)$"
      ]
    },
    {
      "metadata": {
        "id": "nnN1GE_a8Pwv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part II: "
      ]
    },
    {
      "metadata": {
        "id": "3F9cReT9EXK0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "cellView": "both",
        "outputId": "9a3a6c7d-6a58-43f5-ee82-3b2c25ae6462"
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.legend_handler import HandlerLine2D\n",
        "\n",
        "# Red\n",
        "def sigmoid(x):\n",
        "  return 1/( 1 + np.exp(-x))\n",
        "\n",
        "# Blue\n",
        "def ReLu(x):\n",
        "  return x * (x > 0)\n",
        "\n",
        "# Green\n",
        "def tanh(x):\n",
        "  return ( np.exp(x) - np.exp(-x) ) / ( np.exp(x) + np.exp(-x) )\n",
        "\n",
        "# Plot them!\n",
        "t = np.arange(-5., 5., 0.2)\n",
        "sig, = plt.plot(t, sigmoid(t), 'r', label='Sigmoid')\n",
        "re, = plt.plot(t, ReLu(t), 'b', label='ReLu')\n",
        "tan, = plt.plot(t, tanh(t), 'g', label='tanh')\n",
        "plt.legend(handler_map={sig: HandlerLine2D(numpoints=4)})\n",
        "plt.axis([-2, 2, -1, 1])\n",
        "plt.title(\"Activation Functions\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEHCAYAAACumTGlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ne6IAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fee12653dd8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "ZMbvuEhax33a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Taking the Derivatives of Activation Functions\n",
        "\n",
        "activation functions are chosen for having nice properties. These nice properties sometimes include smoothness (i.e. differentiability), or how easy it is to compute the derivative, or restricting the range to certain values (e.g. $(0,1)$)"
      ]
    },
    {
      "metadata": {
        "id": "yzTWKH21C1BE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The Sigmoid Function\n",
        "\n",
        "Let's write the sigmoid function $\\sigma (x)$ as the composition of two functions, $f$ and $g$:\n",
        "\n",
        "$\\qquad f(x) = x^{-1} \\qquad$ and $\\qquad g(x) = 1 + Ce^{-kx}$ \n",
        "\n",
        "Where $C$ and $k$ are arbitrary constants. The composition of these two functions is given by: \n",
        "\n",
        "$\\qquad (f \\circ g)(x) = (1 + Ce^{-kx})^{-1} = \\frac{1}{1 + Ce^{-kx}}$\n",
        "\n",
        "To take this composite function's derivative, we can apply the definition of chain rule. By the power rule, we know $f'(x) = -(x)^{-2}$, while the derivative of the exponential is given by $g'(x) =-kCe^{-kx}$, thus:\n",
        "\n",
        "$\\qquad (f \\circ g)'(x) = f'(g(x))g'(x) = -(1+Ce^{-kx})^{-2}(-kCe^{-kx})$\n",
        "\n",
        "**Simplifying the Above Expression:** In its current form, none of the nice properties of the derivative of a sigmoid are apparent. If we clean up our $(f \\circ g)'(x)$, we will find we can write the derivative of a sigmoid, $\\sigma'(x)$, *in terms of $\\sigma(x)$ itself!* And that's a very nice property. Allow me to demonstrate: First, we expand some terms:\n",
        "\n",
        "$\\qquad  \\sigma'(x) = -(1+Ce^{-kx})^{-2}(-kCe^{-kx}) = k\\frac{Ce^{-kx}}{(1+Ce^{-kx})(1+Ce^{-kx})}$\n",
        "\n",
        "Now we will be sneaky and add nothing to the numerator, in the form of $1-1$, \n",
        "\n",
        "$\\qquad \\sigma'(x) = k\\frac{1+ Ce^{-kx}-1}{(1+Ce^{-kx})(1+Ce^{-kx})}$\n",
        "\n",
        "which allows us to then separate out the expression into two fractions:\n",
        "\n",
        "$\\qquad  \\sigma'(x) = k\\big(\\frac{1+ Ce^{-kx}}{(1+Ce^{-kx})(1+Ce^{-kx})} -  \\frac{1}{(1+Ce^{-kx})(1+Ce^{-kx})}\\big)$\n",
        "\n",
        "At this point we notice two things; first, that the first fraction can be simplified to $1/(1+Ce^{-kx})$. Second, that when simplified, this expression is in fact the sigmoid function itself. Third (okay yes we should've noticed three things), the second fraction is this same sigmoid function squared! Thus:\n",
        "\n",
        "$\\qquad  \\sigma'(x) = k\\big(\\frac{1}{1+Ce^{-kx}} -  \\frac{1}{(1+Ce^{-kx})(1+Ce^{-kx})}\\big)$\n",
        "\n",
        "$\\qquad  \\sigma'(x) = k\\big(\\sigma(x) -  \\sigma(x)^2\\big)$\n",
        "\n",
        "$\\qquad  \\sigma'(x) = k\\sigma(x)(1 -  \\sigma(x))$"
      ]
    },
    {
      "metadata": {
        "id": "v9CvAN65x5PK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The ReLU Function\n",
        "\n",
        "The rectifier (sometimes called the Rectified Linear Unit) is another activation function. It is a piece-wise function of the form:\n",
        "\n",
        "\n",
        "$f(x) = \\begin{cases} \n",
        "      mx & x \\geq 0 \\\\\n",
        "      0 & x < 0 \n",
        "   \\end{cases}$\n",
        "\n",
        "This makes $f'(x)$ trivially easy to compute, with the one caveat that the function isn't differentiable at zero. But this is easily, um, rectified by just arbitrarily assigning $f'(0) = 0$ or  $f'(0) = m$:\n",
        "\n",
        "$f'(x) = \\begin{cases} \n",
        "      m & x \\geq 0 \\\\\n",
        "      0 & x < 0 \n",
        "   \\end{cases}\\qquad$ or $\\qquad f'(x) = \\begin{cases} \n",
        "      m & x > 0 \\\\\n",
        "      0 & x \\leq 0 \n",
        "   \\end{cases}$\n",
        "   \n",
        "ReLU is currently very popular because it is not only easier to compute than $\\sigma(x)$, it also tends to perform better. "
      ]
    },
    {
      "metadata": {
        "id": "GkOvjeZs0_Lt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The Tanh Function\n",
        "\n",
        "The tanh function (pronounced \"tanch\") is composed of the hyperbolic sinh (\"sinch\") and cosh (\"cosh\") functions:\n",
        "\n",
        "$\\sinh(x) = e^{kx} - e^{-kx}$\n",
        "\n",
        "$\\cosh(x) = e^{kx} + e^{-kx}$\n",
        "\n",
        "$\\tanh(x) = \\frac{\\sinh(x)}{\\cosh(x)}  =\\frac{e^{kx} - e^{-kx}}{e^{kx} + e^{-kx}}$\n",
        "\n",
        "*Exercise: show that* $\\tanh'(x) = k(1-\\tanh^2(x))$ (*hint: the hyperbolic functions have many identities analogous to the trig functions, such as* $\\cosh^2(x) - \\sinh^2(x) = 1$ *and *$\\frac{1}{\\cosh^2(x)} = 1 - \\tanh^2(x)$ *that might be useful*):"
      ]
    },
    {
      "metadata": {
        "id": "HDCDUngf8dTl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Solution:**"
      ]
    },
    {
      "metadata": {
        "id": "G87k-6_i9plb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Cost Function as a Composition of Activation Functions\n",
        "\n",
        "Now we can finally consider an actual neural network:\n",
        ">>>>>>> ![alt text](https://scontent.fmkc1-1.fna.fbcdn.net/v/t1.15752-9/39741849_1850782111704901_394883712715587584_n.png?_nc_cat=0&oh=2fc735967d7230a009a80729ac4bf750&oe=5BF416D8) \n",
        "\n",
        "Two hidden layers, two neurons per layer. If we suppose a sigmoidal activation function and a binary response variable, we could describe each neuron with the following equations:\n",
        "\n",
        "$\\qquad h_1 = \\frac{1}{1 + e^{-(\\beta_{11}x_1+\\beta_{12}x_2+\\beta_{13})}}$\n",
        "\n",
        "$\\qquad h_2 = \\frac{1}{1 + e^{-(\\beta_{21}x_1+\\beta_{22}x_2+\\beta_{23})}}$\n",
        "\n",
        "$\\qquad h_3 = \\frac{1}{1 + e^{-(\\beta_{31}h_1+\\beta_{32}h_2+\\beta_{33})}}$\n",
        "\n",
        "$\\qquad h_4 = \\frac{1}{1 + e^{-(\\beta_{41}h_1+\\beta_{42}h_2+\\beta_{43})}}$\n",
        "\n",
        "$\\qquad \\hat{y} = \\frac{1}{1 + e^{-(\\beta_{1}h_3+\\beta_{2}h_4+\\beta_{3})}}$\n",
        "\n",
        "\n",
        "And the cost function can still be written as:\n",
        "\n",
        "$$\\epsilon = \\sum_i^n (y_i - \\hat{y}_i(\\beta))^2$$\n",
        "\n",
        "but really, we know this to be a compacted expression of something much, much uglier, like:\n",
        "\n",
        "$$\\epsilon = \\sum_i^n \\Bigg(y_i - \\frac{1}{1 + e^{-(\\beta_{1}\\big(\\frac{1}{(1 + e^{-(\\beta_{31}(\\frac{1}{1 + e^{-(\\beta_{11}x_1+\\beta_{12}x_2+\\beta_{13})}})+\\beta_{32}(\\frac{1}{1 + e^{-(\\beta_{21}x_1+\\beta_{22}x_2+\\beta_{23})}})+\\beta_{33})})}\\big)  +\\beta_{2}\\big( \\frac{1}{(1 + e^{-(\\beta_{41}(\\frac{1}{1 + e^{-(\\beta_{11}x_1+\\beta_{12}x_2+\\beta_{13})}})+\\beta_{42}(\\frac{1}{1 + e^{-(\\beta_{21}x_1+\\beta_{22}x_2+\\beta_{23})}})+\\beta_{43})}} \\big)+\\beta_{3})})}\\Bigg)^2$$\n",
        "\n",
        "This is an intractible hot mess and not even worth trying to parse. We know the ultimate goal of gradient descent is to repeatedly find $\\nabla _\\beta \\epsilon$ to incrementally improve our weights, but this expression of $\\epsilon$ is just not the way to go. Let's never look at this again. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "QkeSJLQMFYiw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The Gradient of a Neural Network\n",
        "\n",
        "The good news is, we don't need that expression. We don't even need to concern ourselves with the summation directly: if we only take the gradient of the expression $(y_1 - \\hat{y}_1)^2$, we can immediately generalize this by taking the sum of the gradient across all the data. This is because the derivative of a sum is equal to the sum of the derivatives (for some generic $u$ and $v$):\n",
        "\n",
        "$\\qquad \\frac{d}{dx}(u+v) = \\frac{du}{dx}+\\frac{dv}{dx}$\n",
        "\n",
        "Of course, this implies  we intend to take the partial derivatives with respect to all $\\beta$, none of which are currently present in the compacted expression of $\\epsilon$. We saw the loss function is in essence just a bunch of nested sigma functions. Instead of working with it directly, we can work backwards (it's called back propagation for a reason) and slowly expand $\\epsilon$ to contain the weigh whose partial derivative we wish to calculate. \n",
        "\n",
        "**The partial derivative with respect to $\\beta_1$:** \n",
        "\n",
        "Let's first compute $\\frac{\\partial \\epsilon}{\\partial \\beta_1}$. By chainrule, we know this can be rewritten as:\n",
        "\n",
        "$\\frac{\\partial \\epsilon}{\\partial \\beta_1} = \\frac{\\partial \\epsilon}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial \\beta_1}$\n",
        "\n",
        "We compute each separately. The first is fairly simple: \n",
        "\n",
        "$\\frac{\\partial \\epsilon}{\\partial \\hat{y}} = -2(y - \\hat{y})$\n",
        "\n",
        "The important thing now, before we compute $\\frac{\\partial \\epsilon}{\\partial \\beta_1}$, is to draw the analogy between the derivative we took earlier, and the derivative of specifically $\\hat{y}$. Both ar sigmoids. Earlier, our sigmoid was:\n",
        "\n",
        " $\\qquad \\sigma(x) = (1 + Ce^{-kx})^{-1}$\n",
        " \n",
        " and its derivative (I use Leibnez notation here):\n",
        " \n",
        "  $\\qquad  \\frac{{d}\\sigma(x)}{{d}x} = k\\sigma(x)(1 -  \\sigma(x))$\n",
        " \n",
        " So let's make $\\hat{y}$ look like $\\sigma(x)$. Instead of writing $e^{-(\\beta_{1}h_3+\\beta_{2}h_4+\\beta_{3})}$, we use the multiplicative law of exponents to move $\\beta_3$ and $\\beta_{2}h_4$ out in front of $e$:\n",
        " \n",
        " $\\qquad e^{-(\\beta_{1}h_3+\\beta_{2}h_4+\\beta_{3})} = e^{-(\\beta_{2}h_4+\\beta_{3})}e^{-(\\beta_{1}h_3)}$\n",
        " \n",
        " This is great news because, like $C$, $(e^{-(\\beta_{2}h_4+\\beta_{3})})$ is treated as a constant when we take $\\frac{\\partial \\epsilon}{\\partial \\beta_1}$.  Thus we can rewrite $\\hat{y}$ such that it is functionally equivalent to our expression of $\\sigma(x)$! That is, where $x = \\beta_1$, $k = h_3$, and $C = e^{-(\\beta_{2}h_4+\\beta_{3})}$. Thus:\n",
        " \n",
        "  $\\qquad \\hat{y} = (1 + e^{-(\\beta_{2}h_4+\\beta_{3})}e^{-h_3\\beta_1})^{-1}$\n",
        " \n",
        " And its derivative, with respect to $\\beta_1$:\n",
        " \n",
        " $\\qquad  \\frac{{\\partial}\\hat{y}}{{\\partial}\\beta_1} = h_3\\hat{y}(1 -  \\hat{y})$\n",
        " \n",
        " Finally, we put things together:\n",
        " \n",
        " $$\\frac{\\partial \\epsilon}{\\partial \\beta_1} = -2(y - \\hat{y})h_3\\hat{y}(1 -  \\hat{y})$$\n",
        " \n",
        " \n",
        " **The partial derivative with respect to $\\beta_2$:**\n",
        " \n",
        " \n",
        " what about the next derivative?\n",
        " \n",
        "$\\qquad \\frac{\\partial \\epsilon}{\\partial \\beta_2} = \\frac{\\partial \\epsilon}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial \\beta_2}$\n",
        "\n",
        " $\\qquad \\hat{y} = (1 + e^{-(\\beta_{1}h_3+\\beta_{3})}e^{-h_4\\beta_2})^{-1}$\n",
        " \n",
        "  $\\qquad \\frac{\\partial \\epsilon}{\\partial \\beta_2} = -2(y - \\hat{y})h_4\\hat{y}(1 -  \\hat{y})$\n",
        "  \n",
        "   **The partial derivative with respect to $\\beta_3$:**\n",
        "   \n",
        "   The partial derivatives with respect to the bias terms are especially nice because they have no coefficient:\n",
        "   \n",
        "  $\\frac{\\partial \\epsilon}{\\partial \\beta_3} = \\frac{\\partial \\epsilon}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial \\beta_3} = -2(y - \\hat{y})\\hat{y}(1 -  \\hat{y})$\n",
        " \n"
      ]
    },
    {
      "metadata": {
        "id": "2TRppM0uZVLX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$\\frac{\\partial \\epsilon}{\\partial \\beta_{41}} = \\frac{\\partial \\epsilon}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial h_4}\\frac{\\partial h_4}{\\partial \\beta_{41}}$\n",
        "\n",
        "$\\frac{\\partial \\epsilon}{\\partial \\hat{y}} = -2(y-\\hat{y})$\n",
        "\n",
        "$\\frac{\\partial \\hat{y}}{\\partial h_4} = \\beta_2\\hat{y}(1-\\hat{y})$\n",
        "\n",
        "$\\frac{\\partial h_4}{\\partial \\beta_{41}} = h_1h_4(1-h_4)$\n",
        "\n",
        " $\\sigma(x) = (1 + Ce^{-kx})^{-1}$\n",
        "\n",
        "  $\\frac{{d}\\sigma(x)}{{d}x} = k\\sigma(x)(1 -  \\sigma(x))$\n",
        "  \n",
        "  $h_4 = (1+e^{-(\\beta_{41}h_1+ \\beta_{42}h_2 +\\beta_{43})})$\n",
        "  \n",
        "  $e^{-(\\beta_{41}h_1+ \\beta_{42}h_2 +\\beta_{43})} =e^{(\\beta_{42}h_2 +\\beta_{43})}e^{-h_1\\beta_{41}}$\n",
        "  \n",
        "  \n",
        "  >>> $h_4 = (1 + e^{(\\beta_{42}h_2 +\\beta_{43})}e^{-h_1\\beta_{41}})^{-1}$\n",
        "  \n",
        "  \n",
        "  >>>  $\\sigma(x) = (1 + Ce^{-kx})^{-1}$\n",
        "  \n",
        "  $\\frac{\\partial \\epsilon}{\\partial \\beta_{41}} =  -2(y-\\hat{y})\\beta_2\\hat{y}(1-\\hat{y}) h_1h_4(1-h_4)$\n",
        "  \n",
        "  "
      ]
    },
    {
      "metadata": {
        "id": "O5-_Z2IxcIfV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gradient_descent():\n",
        "  while not time_to_stop: #these are epochs\n",
        "    del_beta = 0\n",
        "    for i in data:\n",
        "      values = forward_propagate(i)\n",
        "      del_beta_i = back_propagate(values)\n",
        "      \n",
        "      # del_error_del_beta_1_i = -2*(y-haty)*h_3*haty*(1-haty)\n",
        "      del_beta += del_beta_i\n",
        "    beta += - alpha*del_beta \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yBgJ7zlEg71-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Your Task:** compute all of the partial derivatives. Use them to perform gradient descent on the neural network pictured above."
      ]
    },
    {
      "metadata": {
        "id": "8hAJOqcOhLFp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = [[0,0],[0,1],[1,0],[1,1]]\n",
        "Y = [[0,1,1,0]]"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}